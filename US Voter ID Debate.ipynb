{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voter ID Debate\n",
    "\n",
    "This is a Notebook file with analyses of US Congressional Debates over __voter ID__. The analyses conducted are inductive and bottom up, meaning that the interest lies in recognising and interpreting patterns emerging from the data. \n",
    "\n",
    "Specifically these analyses aim to identify:\n",
    "1. the topics voter ID is linked to.\n",
    "2. the ways in which voter ID is framed by Democratic and Republican speakers.\n",
    "3. how frames/topics around voter ID change over time.\n",
    "\n",
    "It is split into the sections below:\n",
    "- _Preprocessing_\n",
    "    * Clean up: remove punctuation, convert to lower case\n",
    "    * Remove common stop words\n",
    "    * lemmatisation\n",
    "    \n",
    "- _Descriptive Statistics_\n",
    "    * Most common words and wordcloud\n",
    "    * Most common n-grams \n",
    "    * Number of speeches per Party and Congress\n",
    "\n",
    "- Reducing corpus size and noise\n",
    "    * remove stopwords using TF-IFG scores: to remove words that are not meaningful\n",
    "    * Filter extremes: remove words that occur in less that n=5 documents or in more than 50% of all documents\n",
    "    * Join common bigrams\n",
    " \n",
    "- _Analyses_\n",
    "    * Word embedding model (using the Word2vec implementation provided by the gensim package) and visualisation\n",
    "    * Co-occurrence network\n",
    "    * Temporal dimension: dynamic word embedding model, word embedding model per Congress\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "import re, nltk, numpy, matplotlib\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First we load the data, inspect it and remove columns we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"US_Debates_ngrams.csv\")\n",
    "data.head()\n",
    "df = data.drop(columns=['Title', 'State', 'Volume', 'Where', 'Unnamed: 9', 'Unnamed: 11', 'Order']) # Remove columns\n",
    "df. dropna() # Drop NA rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =df['text'].apply(str) \n",
    "tokens = [\" \".join(strings.split()) for strings in text] \n",
    "string = \" \".join(tokens)\n",
    "tokenized = word_tokenize(string)\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now we can start preprocessing, by tokenising the corpus, removing puctuation and common stopwords, and converting the text into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = 'english.stop.txt' # Stopword list\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split()) \n",
    "        \n",
    "stop_words = stop_words  \n",
    "\n",
    "def preprocess(raw_text):\n",
    "    \n",
    "    #regular expression keeping only letters \n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    letters_only_text = letters_only_text.replace(\"voter fraud\", \"voter_fraud\")\n",
    "    letters_only_text = letters_only_text.replace(\"supreme court\", \"supreme_court\")\n",
    "    letters_only_text = letters_only_text.replace(\"madam speaker\", \"madam_speaker\")\n",
    "    letters_only_text = letters_only_text.replace(\"mr speaker\", \"mr_speaker\")\n",
    "    letters_only_text = letters_only_text.replace(\"african american\", \"african_american\")\n",
    "    letters_only_text = letters_only_text.replace(\"north carolina\", \"north_carolina\")\n",
    "    letters_only_text = letters_only_text.replace(\"united state\", \"united_states\")\n",
    "\n",
    "     # convert to lower case and split into words -> convert string into list\n",
    "    words = letters_only_text.lower().split()\n",
    "    words=[\" \".join(words.split()) for words in words]   # remove double spaces by splitting the strings into words and joining these words again\n",
    "\n",
    "    cleaned_words = []\n",
    "    lemmatizer = WordNetLemmatizer() #plug in here any other stemmer or lemmatiser you want to try out\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # lemmatise words\n",
    "    lemmatised_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = nltk.stem.WordNetLemmatizer().lemmatize(word)  \n",
    "        lemmatised_words.append(word)\n",
    "\n",
    "    # converting list back to string\n",
    "    return \" \".join(lemmatised_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence demonstrate preprocessing function work madam_speaker'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"this is a sentence to demonstrate how the preprocessing function works...! madam speaker\"\n",
    "\n",
    "preprocess(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Party</th>\n",
       "      <th>text</th>\n",
       "      <th>Congress</th>\n",
       "      <th>we</th>\n",
       "      <th>prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thursday, January 3, 2013</td>\n",
       "      <td>Ms. SLAUGHTER</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Under the cover of a cynical and untruthful cl...</td>\n",
       "      <td>113th</td>\n",
       "      <td>cover cynical untruthful claim voter fraud thr...</td>\n",
       "      <td>cover cynical untruthful claim voter_fraud thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As a Member of this body, I firmly believe th...</td>\n",
       "      <td>113th</td>\n",
       "      <td>member body, firmly protect right eligible cit...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>There have been several restrictive voting bi...</td>\n",
       "      <td>113th</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday, February 14, 2014</td>\n",
       "      <td>Mr. VEASEY</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As oral arguments are being prepared for the F...</td>\n",
       "      <td>113th</td>\n",
       "      <td>oral argument prepared february 27 u.s. suprem...</td>\n",
       "      <td>oral argument prepared february supreme court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monday, February 25, 2013</td>\n",
       "      <td>Mr. JEFFRIES</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Perhaps the most relevant example of why secti...</td>\n",
       "      <td>113th</td>\n",
       "      <td>relevant section 5 continues relevant due plac...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date           Speaker       Party  \\\n",
       "0    Thursday, January 3, 2013     Ms. SLAUGHTER  Democratic   \n",
       "1  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "2  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "3  Thursday, February 14, 2014        Mr. VEASEY  Democratic   \n",
       "4    Monday, February 25, 2013      Mr. JEFFRIES  Democratic   \n",
       "\n",
       "                                                text Congress  \\\n",
       "0  Under the cover of a cynical and untruthful cl...    113th   \n",
       "1   As a Member of this body, I firmly believe th...    113th   \n",
       "2   There have been several restrictive voting bi...    113th   \n",
       "3  As oral arguments are being prepared for the F...    113th   \n",
       "4  Perhaps the most relevant example of why secti...    113th   \n",
       "\n",
       "                                                  we  \\\n",
       "0  cover cynical untruthful claim voter fraud thr...   \n",
       "1  member body, firmly protect right eligible cit...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  oral argument prepared february 27 u.s. suprem...   \n",
       "4  relevant section 5 continues relevant due plac...   \n",
       "\n",
       "                                                prep  \n",
       "0  cover cynical untruthful claim voter_fraud thr...  \n",
       "1  member body firmly protect right eligible citi...  \n",
       "2  restrictive voting bill considered approved st...  \n",
       "3  oral argument prepared february supreme court ...  \n",
       "4  relevant section continues relevant due place ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']=df['text'].apply(str) # Converting text column into string so that preprocessing works\n",
    "\n",
    "df['prep'] = df['text'].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking amount of words after preprocessing\n",
    "text =df['prep'].apply(str) \n",
    "tokens = [\" \".join(strings.split()) for strings in text] \n",
    "string = \" \".join(tokens)\n",
    "tokenized = word_tokenize(string)\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dem =  df['Party']=='Democratic'\n",
    "dem = df[is_dem]\n",
    "len(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rep =  df['Party']=='Republican'\n",
    "rep = df[is_rep]\n",
    "len(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =dem['prep'].apply(str) \n",
    "tokens = [\" \".join(strings.split()) for strings in text] \n",
    "string = \" \".join(tokens)\n",
    "tokenized = word_tokenize(string)\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessed.csv\") # save output to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code to find and remove frequent and infrequent words - I wanted to include these as stopwords but its not ideal as most_common only works with n and not percentages, so its best to use CountVectorizer for this\n",
    "\n",
    "# Finding and removing rare words\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"stopfreqrare\"] = df[\"prep\"].apply(lambda text: remove_rarewords(text))\n",
    "df.head()\n",
    "\n",
    "# Remove very frequent words\n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"stopfreq\"] = df[\"prep\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>state</td>\n",
       "      <td>3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vote</td>\n",
       "      <td>2986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>election</td>\n",
       "      <td>2907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voter</td>\n",
       "      <td>2747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voting</td>\n",
       "      <td>2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>american</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>people</td>\n",
       "      <td>1895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>law</td>\n",
       "      <td>1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bill</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>right</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>court</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ballot</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>year</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>democrat</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>president</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>republican</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vra</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>speaker</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>id</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>make</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>democracy</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>federal</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mr</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>time</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>country</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>act</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>colleague</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>day</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>today</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>congress</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  freq\n",
       "0        state  3182\n",
       "1         vote  2986\n",
       "2     election  2907\n",
       "3        voter  2747\n",
       "4       voting  2371\n",
       "5     american  1988\n",
       "6       people  1895\n",
       "7          law  1759\n",
       "8         bill  1338\n",
       "9        right  1186\n",
       "10       court  1033\n",
       "11      ballot  1012\n",
       "12        year   951\n",
       "13    democrat   918\n",
       "14   president   902\n",
       "15  republican   894\n",
       "16         vra   867\n",
       "17     speaker   866\n",
       "18          id   865\n",
       "19        make   841\n",
       "20   democracy   785\n",
       "21     federal   783\n",
       "22          mr   756\n",
       "23        time   755\n",
       "24     country   749\n",
       "25         act   690\n",
       "26   colleague   670\n",
       "27         day   669\n",
       "28       today   651\n",
       "29    congress   615"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"prep\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "# cnt = Counter(\" \".join(df[\"prep\"]).split(\" \")).most_common(10) - another way of finding most common words in corpus\n",
    "\n",
    "cnt.most_common(30)\n",
    "cnt_df = pd.DataFrame(cnt.most_common(30))\n",
    "cnt_df.columns=['word', 'freq']\n",
    "cnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying top words\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def scatterplot(df, x_dim, y_dim):\n",
    "  x = df[x_dim]\n",
    "  y = df[y_dim]\n",
    "  fig, ax = plt.subplots(figsize=(10, 10))\n",
    "  #customizes alpha for each dot in the scatter plot\n",
    "  ax.scatter(x, y, alpha=0.70)\n",
    " \n",
    "  #adds a title and axes labels\n",
    "  ax.set_title('Word Frequencies')\n",
    "  ax.set_xlabel('Frequency')\n",
    "  ax.set_ylabel('Word')\n",
    " \n",
    "  #removing top and right borders\n",
    "  ax.spines['top'].set_visible(False)\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  #adds major gridlines\n",
    "  ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "  plt.show()\n",
    "\n",
    "  #LEARN HOW TO SAVE PLOTS, plt.savefig('plot.png') saving black plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatterplot(cnt_df, 'freq', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transmission', 1),\n",
       " ('operator', 1),\n",
       " ('delicate', 1),\n",
       " ('activation', 1),\n",
       " ('obeying', 1),\n",
       " ('brazenly', 1),\n",
       " ('weakness', 1),\n",
       " ('blackmail', 1),\n",
       " ('dormancy', 1),\n",
       " ('capitulate', 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find very infrequent words\n",
    "cnt.most_common()[-10-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "g3xmXBEyVE92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('voter', 'id') 495\n",
      "('id', 'law') 342\n",
      "('cast', 'ballot') 103\n",
      "('early', 'voting') 223\n",
      "('minority', 'voter') 104\n",
      "('election', 'day') 112\n",
      "('voter', 'registration') 224\n",
      "('african', 'american') 391\n",
      "('state', 'local') 100\n",
      "('election', 'law') 155\n",
      "('photo', 'id') 136\n",
      "('voting', 'right') 679\n",
      "('polling', 'place') 131\n",
      "('supreme', 'court') 462\n",
      "('shelby', 'county') 184\n",
      "('voter', 'suppression') 363\n",
      "('year', 'ago') 142\n",
      "('mr', 'speaker') 332\n",
      "('civil', 'right') 214\n",
      "('voting', 'law') 148\n",
      "('united', 'state') 294\n",
      "('john', 'lewis') 205\n",
      "('american', 'vote') 133\n",
      "('people', 'vote') 103\n",
      "('north', 'carolina') 250\n",
      "('ballot', 'box') 108\n",
      "('madam', 'speaker') 455\n",
      "('federal', 'government') 163\n",
      "('state', 'legislature') 163\n",
      "('federal', 'election') 169\n",
      "('american', 'people') 268\n",
      "('right', 'advancement') 108\n",
      "('advancement', 'act') 108\n",
      "('presidential', 'election') 103\n",
      "('vote', 'mail') 103\n",
      "('president', 'trump') 131\n",
      "('mr', 'farr') 109\n",
      "('ballot', 'harvesting') 118\n",
      "('people', 'act') 146\n",
      "('postal', 'service') 153\n"
     ]
    }
   ],
   "source": [
    "# Find top bigrams with a frequency above 100\n",
    "from nltk.util import ngrams\n",
    "n_gram = 2\n",
    "n_gram_dic = dict(Counter(ngrams(\" \".join(df[\"bigram\"]).split(), n_gram))) # Finding ngrams in corpus\n",
    "\n",
    "for i in n_gram_dic:\n",
    "    if n_gram_dic[i] >= 100:\n",
    "        print(i, n_gram_dic[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "JLItn_NCr_YD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(voting, right)</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(voter, id)</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(supreme, court)</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(madam, speaker)</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(african, american)</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(voter, suppression)</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(id, law)</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(mr, speaker)</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(united, state)</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(american, people)</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(north, carolina)</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(voter, registration)</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(early, voting)</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(civil, right)</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(john, lewis)</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(shelby, county)</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(federal, election)</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(federal, government)</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(state, legislature)</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(election, law)</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(postal, service)</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(voting, law)</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(people, act)</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(year, ago)</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(photo, id)</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(american, vote)</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(polling, place)</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(president, trump)</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(ballot, harvesting)</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(election, day)</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(mr, farr)</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(ballot, box)</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(right, advancement)</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(advancement, act)</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(minority, voter)</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(cast, ballot)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(people, vote)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(presidential, election)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(vote, mail)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(state, local)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ngram  freq\n",
       "0            (voting, right)   679\n",
       "1                (voter, id)   495\n",
       "2           (supreme, court)   462\n",
       "3           (madam, speaker)   455\n",
       "4        (african, american)   391\n",
       "5       (voter, suppression)   363\n",
       "6                  (id, law)   342\n",
       "7              (mr, speaker)   332\n",
       "8            (united, state)   294\n",
       "9         (american, people)   268\n",
       "10         (north, carolina)   250\n",
       "11     (voter, registration)   224\n",
       "12           (early, voting)   223\n",
       "13            (civil, right)   214\n",
       "14             (john, lewis)   205\n",
       "15          (shelby, county)   184\n",
       "16       (federal, election)   169\n",
       "17     (federal, government)   163\n",
       "18      (state, legislature)   163\n",
       "19           (election, law)   155\n",
       "20         (postal, service)   153\n",
       "21             (voting, law)   148\n",
       "22             (people, act)   146\n",
       "23               (year, ago)   142\n",
       "24               (photo, id)   136\n",
       "25          (american, vote)   133\n",
       "26          (polling, place)   131\n",
       "27        (president, trump)   131\n",
       "28      (ballot, harvesting)   118\n",
       "29           (election, day)   112\n",
       "30                (mr, farr)   109\n",
       "31             (ballot, box)   108\n",
       "32      (right, advancement)   108\n",
       "33        (advancement, act)   108\n",
       "34         (minority, voter)   104\n",
       "35            (cast, ballot)   103\n",
       "36            (people, vote)   103\n",
       "37  (presidential, election)   103\n",
       "38              (vote, mail)   103\n",
       "39            (state, local)   100"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting ngrams by frequency\n",
    "from nltk.util import ngrams\n",
    "n_gram = 2\n",
    "n_gram_dic = Counter(ngrams(\" \".join(df[\"bigram\"]).split(), n_gram)).most_common(40) # 20 most common ngrams\n",
    "ngram = pd.DataFrame(n_gram_dic)\n",
    "ngram.columns=['ngram', 'freq']\n",
    "ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know common bigrams we can go back to preprocessing and join these in the text so that they are subsequently treated as 1 token instead of 2. \n",
    "The following bigrams are joined:\n",
    "- Supreme court\n",
    "- Madam speaker\n",
    "- Mr Speaker\n",
    "- United States\n",
    "- African American\n",
    "- John Lewis\n",
    "\n",
    "Mr Speaker and Madam speaker are removed from the corpus as they add little meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bigrams(raw_text):\n",
    "    letters_only_text = raw_text.replace(\"supreme court\", \"supreme_court\")\n",
    "    letters_only_text = letters_only_text.replace(\"madam speaker\", \"madam_speaker\")\n",
    "    letters_only_text = letters_only_text.replace(\"mr speaker\", \"mr_speaker\")\n",
    "    letters_only_text = letters_only_text.replace(\"african american\", \"african_american\")\n",
    "    letters_only_text = letters_only_text.replace(\"north carolina\", \"north_carolina\")\n",
    "     # convert to lower case and split into words -> convert string into list\n",
    "    words = letters_only_text.lower().split()\n",
    "    words=[\" \".join(words.split()) for words in words]   # remove double spaces by splitting the strings into words and joining these words again\n",
    "\n",
    "    cleaned_words = []\n",
    "    lemmatizer = WordNetLemmatizer() #plug in here any other stemmer or lemmatiser you want to try out\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # lemmatise words\n",
    "    lemmatised_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = nltk.stem.WordNetLemmatizer().lemmatize(word)  \n",
    "        lemmatised_words.append(word)\n",
    "\n",
    "    # converting list back to string\n",
    "    return \" \".join(lemmatised_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Party</th>\n",
       "      <th>text</th>\n",
       "      <th>Congress</th>\n",
       "      <th>we</th>\n",
       "      <th>prep</th>\n",
       "      <th>freq</th>\n",
       "      <th>bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thursday, January 3, 2013</td>\n",
       "      <td>Ms. SLAUGHTER</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Under the cover of a cynical and untruthful cl...</td>\n",
       "      <td>113th</td>\n",
       "      <td>cover cynical untruthful claim voter fraud thr...</td>\n",
       "      <td>cover cynical untruthful claim voter_fraud thr...</td>\n",
       "      <td>cover cynical claim voter_fraud threat democra...</td>\n",
       "      <td>cover cynical untruthful claim voter_fraud thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As a Member of this body, I firmly believe th...</td>\n",
       "      <td>113th</td>\n",
       "      <td>member body, firmly protect right eligible cit...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>There have been several restrictive voting bi...</td>\n",
       "      <td>113th</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday, February 14, 2014</td>\n",
       "      <td>Mr. VEASEY</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As oral arguments are being prepared for the F...</td>\n",
       "      <td>113th</td>\n",
       "      <td>oral argument prepared february 27 u.s. suprem...</td>\n",
       "      <td>oral argument prepared february supreme court ...</td>\n",
       "      <td>argument prepared february supreme court heari...</td>\n",
       "      <td>oral argument prepared february supreme court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monday, February 25, 2013</td>\n",
       "      <td>Mr. JEFFRIES</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Perhaps the most relevant example of why secti...</td>\n",
       "      <td>113th</td>\n",
       "      <td>relevant section 5 continues relevant due plac...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date           Speaker       Party  \\\n",
       "0    Thursday, January 3, 2013     Ms. SLAUGHTER  Democratic   \n",
       "1  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "2  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "3  Thursday, February 14, 2014        Mr. VEASEY  Democratic   \n",
       "4    Monday, February 25, 2013      Mr. JEFFRIES  Democratic   \n",
       "\n",
       "                                                text Congress  \\\n",
       "0  Under the cover of a cynical and untruthful cl...    113th   \n",
       "1   As a Member of this body, I firmly believe th...    113th   \n",
       "2   There have been several restrictive voting bi...    113th   \n",
       "3  As oral arguments are being prepared for the F...    113th   \n",
       "4  Perhaps the most relevant example of why secti...    113th   \n",
       "\n",
       "                                                  we  \\\n",
       "0  cover cynical untruthful claim voter fraud thr...   \n",
       "1  member body, firmly protect right eligible cit...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  oral argument prepared february 27 u.s. suprem...   \n",
       "4  relevant section 5 continues relevant due plac...   \n",
       "\n",
       "                                                prep  \\\n",
       "0  cover cynical untruthful claim voter_fraud thr...   \n",
       "1  member body firmly protect right eligible citi...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  oral argument prepared february supreme court ...   \n",
       "4  relevant section continues relevant due place ...   \n",
       "\n",
       "                                                freq  \\\n",
       "0  cover cynical claim voter_fraud threat democra...   \n",
       "1  member body firmly protect right eligible citi...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  argument prepared february supreme court heari...   \n",
       "4  relevant section continues relevant due place ...   \n",
       "\n",
       "                                              bigram  \n",
       "0  cover cynical untruthful claim voter_fraud thr...  \n",
       "1  member body firmly protect right eligible citi...  \n",
       "2  restrictive voting bill considered approved st...  \n",
       "3  oral argument prepared february supreme court ...  \n",
       "4  relevant section continues relevant due place ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']=df['text'].apply(str) # Converting text column into string so that preprocessing works\n",
    "\n",
    "df['bigram'] = df['text'].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Party</th>\n",
       "      <th>text</th>\n",
       "      <th>Congress</th>\n",
       "      <th>we</th>\n",
       "      <th>prep</th>\n",
       "      <th>freq</th>\n",
       "      <th>bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thursday, January 3, 2013</td>\n",
       "      <td>Ms. SLAUGHTER</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Under the cover of a cynical and untruthful cl...</td>\n",
       "      <td>113th</td>\n",
       "      <td>cover cynical untruthful claim voter fraud thr...</td>\n",
       "      <td>cover cynical untruthful claim voter_fraud thr...</td>\n",
       "      <td>cover cynical claim voter_fraud threat democra...</td>\n",
       "      <td>cover cynical untruthful claim voter_fraud thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As a Member of this body, I firmly believe th...</td>\n",
       "      <td>113th</td>\n",
       "      <td>member body, firmly protect right eligible cit...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "      <td>member body firmly protect right eligible citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday, February 14, 2013</td>\n",
       "      <td>Ms. JACKSON LEE</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>There have been several restrictive voting bi...</td>\n",
       "      <td>113th</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "      <td>restrictive voting bill considered approved st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday, February 14, 2014</td>\n",
       "      <td>Mr. VEASEY</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>As oral arguments are being prepared for the F...</td>\n",
       "      <td>113th</td>\n",
       "      <td>oral argument prepared february 27 u.s. suprem...</td>\n",
       "      <td>oral argument prepared february supreme court ...</td>\n",
       "      <td>argument prepared february supreme court heari...</td>\n",
       "      <td>oral argument prepared february supreme court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monday, February 25, 2013</td>\n",
       "      <td>Mr. JEFFRIES</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Perhaps the most relevant example of why secti...</td>\n",
       "      <td>113th</td>\n",
       "      <td>relevant section 5 continues relevant due plac...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "      <td>relevant section continues relevant due place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date           Speaker       Party  \\\n",
       "0    Thursday, January 3, 2013     Ms. SLAUGHTER  Democratic   \n",
       "1  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "2  Thursday, February 14, 2013   Ms. JACKSON LEE  Democratic   \n",
       "3  Thursday, February 14, 2014        Mr. VEASEY  Democratic   \n",
       "4    Monday, February 25, 2013      Mr. JEFFRIES  Democratic   \n",
       "\n",
       "                                                text Congress  \\\n",
       "0  Under the cover of a cynical and untruthful cl...    113th   \n",
       "1   As a Member of this body, I firmly believe th...    113th   \n",
       "2   There have been several restrictive voting bi...    113th   \n",
       "3  As oral arguments are being prepared for the F...    113th   \n",
       "4  Perhaps the most relevant example of why secti...    113th   \n",
       "\n",
       "                                                  we  \\\n",
       "0  cover cynical untruthful claim voter fraud thr...   \n",
       "1  member body, firmly protect right eligible cit...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  oral argument prepared february 27 u.s. suprem...   \n",
       "4  relevant section 5 continues relevant due plac...   \n",
       "\n",
       "                                                prep  \\\n",
       "0  cover cynical untruthful claim voter_fraud thr...   \n",
       "1  member body firmly protect right eligible citi...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  oral argument prepared february supreme court ...   \n",
       "4  relevant section continues relevant due place ...   \n",
       "\n",
       "                                                freq  \\\n",
       "0  cover cynical claim voter_fraud threat democra...   \n",
       "1  member body firmly protect right eligible citi...   \n",
       "2  restrictive voting bill considered approved st...   \n",
       "3  argument prepared february supreme court heari...   \n",
       "4  relevant section continues relevant due place ...   \n",
       "\n",
       "                                              bigram  \n",
       "0  cover cynical untruthful claim voter_fraud thr...  \n",
       "1  member body firmly protect right eligible citi...  \n",
       "2  restrictive voting bill considered approved st...  \n",
       "3  oral argument prepared february supreme court ...  \n",
       "4  relevant section continues relevant due place ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering words that appear less than 3 times overall, by (1) Splitting sentences into words, (2) Computing global word frequency, (3) Filtering words based on computed frequencies, (4) Joining and re-assigning\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# split words into lists\n",
    "v = df['prep'].str.split().tolist() # [s.split() for s in df['Col2'].tolist()]\n",
    "# compute global word frequency\n",
    "c = Counter(chain.from_iterable(v))\n",
    "# filter, join, and re-assign\n",
    "df['freq'] = [' '.join([j for j in i if c[j] > 3]) for i in v]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = \" \".join(df['bigram'])\n",
    "len(re.findall(\"supreme_court\",joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (1.21.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Using legacy 'setup.py install' for wordcloud, since package 'wheel' is not installed.\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\analo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-17038a77\\\\wordcloud_805d38b0b9854fc985ba13476afb028b\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-17038a77\\\\wordcloud_805d38b0b9854fc985ba13476afb028b\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\analo\\AppData\\Local\\Temp\\pip-record-takd893n\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\analo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\analo\\AppData\\Local\\Temp\\pip-install-17038a77\\wordcloud_805d38b0b9854fc985ba13476afb028b\\\n",
      "    Complete output (20 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\analo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-17038a77\\\\wordcloud_805d38b0b9854fc985ba13476afb028b\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-17038a77\\\\wordcloud_805d38b0b9854fc985ba13476afb028b\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\analo\\AppData\\Local\\Temp\\pip-record-takd893n\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\analo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.1.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (1.21.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\analo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Using legacy 'setup.py install' for wordcloud, since package 'wheel' is not installed.\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\analo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-i5nz5sxr\\\\wordcloud_37b943ef2b7b420092b40b1c7d64a1be\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-i5nz5sxr\\\\wordcloud_37b943ef2b7b420092b40b1c7d64a1be\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\analo\\AppData\\Local\\Temp\\pip-record-6hwf_8ki\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\analo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\wordcloud'\n",
      "         cwd: C:\\Users\\analo\\AppData\\Local\\Temp\\pip-install-i5nz5sxr\\wordcloud_37b943ef2b7b420092b40b1c7d64a1be\\\n",
      "    Complete output (20 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\tokenization.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\_version.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\__main__.py -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-3.9\\wordcloud\n",
      "    UPDATING build\\lib.win-amd64-3.9\\wordcloud/_version.py\n",
      "    set build\\lib.win-amd64-3.9\\wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\analo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-i5nz5sxr\\\\wordcloud_37b943ef2b7b420092b40b1c7d64a1be\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\analo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-i5nz5sxr\\\\wordcloud_37b943ef2b7b420092b40b1c7d64a1be\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\analo\\AppData\\Local\\Temp\\pip-record-6hwf_8ki\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\analo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\wordcloud' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15388/2727915997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#nice library to produce wordclouds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install wordcloud'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# if uising a Jupyter notebook, include:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "#nice library to produce wordclouds\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# if uising a Jupyter notebook, include:\n",
    "\n",
    "all_words = '' \n",
    "\n",
    "#looping through all incidents and joining them to one text, to extract most common words\n",
    "for arg in data[\"prep\"]: \n",
    "    tokens = arg.split()  \n",
    "    all_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 1000, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate(all_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (10, 10), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Party'].value_counts() # Counting speeches per Party\n",
    "\n",
    "ax = data['Party'].value_counts().sort_index().plot(kind='bar', fontsize=14, figsize=(7,5))\n",
    "ax.set_title('Party Speech Count\\n', fontsize=20)\n",
    "ax.set_xlabel('Publication', fontsize=18)\n",
    "ax.set_ylabel('Count', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = df['Congress'].value_counts()\n",
    "pd.DataFrame(congress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "O_V4g4yt447N",
    "outputId": "4a655191-b174-4a53-cb8e-cebd3d11b662"
   },
   "outputs": [],
   "source": [
    "ax = df['Congress'].value_counts().sort_index().plot(kind='bar', fontsize=14, figsize=(7,5)) # Counting speeches per Congress\n",
    "ax.set_title('Congress Speech Count\\n', fontsize=20)\n",
    "ax.set_xlabel('Congress', fontsize=18)\n",
    "ax.set_ylabel('Statements', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prep']=df['prep'].apply(str)\n",
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "all_words = '' #looping through all incidents and joining them to one text, to extract most common words\n",
    "for arg in df[\"prep\"]: \n",
    "    tokens = arg.split()  \n",
    "    my_counter.update(tokens)\n",
    "\n",
    "len(my_counter)\n",
    "\n",
    "most_common_three = my_counter.most_common(3)\n",
    "print(most_common_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168878"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =df['freq'].apply(str) \n",
    "tokens = [\" \".join(strings.split()) for strings in text] \n",
    "string = \" \".join(tokens)\n",
    "tokenized = word_tokenize(string)\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tagged = nltk.pos_tag(my_counter)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores instead of word counts as features\n",
    "Use (a) the frequency of each word in the corpus and (b) the number of documents in which the document occurs. So use tf-idf scores (term frequency weighed by the inverse document frequency) instead of raw word counts as features, the stopwords should disappear automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['prep'] # setting 'text' as the preprocessed corpus\n",
    "orig = df['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While cv.fit(...) would only create the vocabulary, cv.fit_transform(...) creates the vocabulary and returns a term-document matrix which is what we want. With this, each column in the matrix represents a word in the vocabulary while each row represents the document in our dataset where the values in this case are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document.\n",
    "\n",
    "We ignore all words that have appeared in 95% of the documents, since those may be unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore words that appear in 95% of documents and in less that 1% of documents,\n",
    "cv=CountVectorizer(max_df=0.95, min_df =0.01)\n",
    "word_count_vector=cv.fit_transform(text)\n",
    "\n",
    "word_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10] #10 words from our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its now time to compute the IDF values. In the code below, we are essentially taking the sparse matrix from CountVectorizer (word_count_vector) to generate the IDF when you invoke tfidf_transformer.fit(...)(see: basic usage example of tfidftransformer and tfidfvectorizer).\n",
    "Once we have our IDF computed, we are now ready to compute TF-IDF and then extract top keywords from the TF-IDF vectors. Now we are going to compute the IDF values. IDF values are be sorted in descending order. The lower the IDF value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid messing things up, I'll call all objects relating to our first model _m1\n",
    "ldainput = [text.split() for text in text]           # convert all strings to list of words\n",
    "id2word = corpora.Dictionary(ldainput)                       # assign a token_id to each word\n",
    "ldacorpus = [id2word.doc2bow(doc) for doc in ldainput] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(ldacorpus, id2word=id2word, num_topics=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf-idf scores to remove corpus related stopwords\n",
    "tfidfcorpus = models.TfidfModel(ldacorpus)\n",
    "lda_m3 = models.ldamodel.LdaModel(corpus=tfidfcorpus[ldacorpus],id2word=id2word,num_topics=10)\n",
    "lda_m3.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter extremes     \n",
    "id2word.filter_extremes(no_below=5)\n",
    "ldacorpus = [id2word.doc2bow(doc) for doc in ldainput]\n",
    "tfidfcorpus = models.TfidfModel(ldacorpus)\n",
    "lda_m4 = models.ldamodel.LdaModel(corpus=tfidfcorpus[ldacorpus],id2word=id2word,num_topics=10)\n",
    "lda_m4.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: preprocess\n",
    "stop_words_file = 'english.stop.txt' # Stopword list\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split()) \n",
    "        \n",
    "stop_words = stop_words  \n",
    "\n",
    "def preprocess_we(raw_text):\n",
    "     # convert to lower case and split into words -> convert string into list\n",
    "    words = raw_text.lower().split()\n",
    "    words=[\" \".join(words.split()) for words in words]   # remove double spaces by splitting the strings into words and joining these words again\n",
    "\n",
    "    cleaned_words = []\n",
    "    lemmatizer = WordNetLemmatizer() #plug in here any other stemmer or lemmatiser you want to try out\n",
    "    \n",
    "    # remove stopwords\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # lemmatise words\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = nltk.stem.WordNetLemmatizer().lemmatize(word)  \n",
    "        stemmed_words.append(word)\n",
    "    \n",
    "    # converting list back to string\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating co-occurrence matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,1), max_df=0.75, min_df =0.05) # You can define your own parameters\n",
    "X = cv.fit_transform(rep['we'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = (X.T * X) # This is the matrix manipulation step\n",
    "Xc.setdiag(0) # We set the diagonals to be zeroes as it's pointless to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we turn it into a pandas DataFrame and export it as a CSV.\n",
    "import pandas as pd\n",
    "names = cv.get_feature_names_out() # This are the entity names (i.e. keywords)\n",
    "rep_net = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
    "rep_net.to_csv('to gephi.csv', sep = ',')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69226dc651b7fbb4cbb68815143d2221db9c2a369a5e9beb9597b16c0cffa010"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
